#install spark 
sudo apt update
sudo apt install default-jre
sudo apt-get install scala
sudo apt-get install git
curl -O http://apache.mirrors.spacedump.net/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
tar xvf spark-2.4.5-bin-hadoop2.7.tgz
sudo mv spark-2.4.5-bin-hadoop2.7/ /opt/spark
vim ~/.bashrc (add: export SPARK_HOME=/opt/spark
		    export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin)
source ~/.bashrc

#set up master node 
cd $SPARK_HOME/conf/
cp spark-env.sh.template spark-env.sh
vim spark-env.sh (set SPARK_MASTER_HOST='192.168.2.154')
cd $SPARK_HOME/sbin (run: ./start-master.sh check logfile!)

#set up worker 
1. create snapshot of master and boot worker from snapshot
2. same as master (see set up master node)
3. create ssh key so that each worker does not have to be started individally 
4. edit conf/slaves on master to inlcude workers (hostnames) 

#install jupyter notebook 
echo "export PYSPARK_PYTHON=python3" >> ~/.bashrc
source ~/.bashrc
sudo apt-get install -y python3-pip
python3 -m pip install pip
python3 -m pip install pyspark==2.4.5 --user
python3 -m pip install pandas --user
python3 -m pip install matplotlib --user
